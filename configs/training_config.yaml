# Training Configuration for Vision-Based Autonomous Driving Agent

# Environment Settings
environment:
  # Available environments: highway-v0, intersection-v0, roundabout-v0, parking-v0
  name: "highway-v0"
  
  # Environment configurations
  config:
    # Road settings
    road:
      num_lanes: 4
      lane_width: 4.0
      speed_limit: 30.0
    
    # Vehicle settings
    vehicles:
      count: 50
      spacing: 2.0
      speed_range: [20.0, 30.0]
    
    # Observation settings
    observation:
      type: "GrayscaleObservation"  # Options: GrayscaleObservation, RGB, Kinematics
      width: 84
      height: 84
      normalize: true
    
    # Action settings
    action:
      type: "ContinuousAction"  # Options: ContinuousAction, DiscreteAction
      acceleration_range: [-5.0, 5.0]
      steering_range: [-0.5, 0.5]
    
    # Simulation settings
    simulation:
      frequency: 15
      policy_frequency: 5
      duration: 40
      offroad_terminal: true
      collision_reward: -100
      right_lane_reward: 1.0
      high_speed_reward: 0.5
      reward_speed_range: [20.0, 30.0]

# Imitation Learning Settings
imitation_learning:
  # Data collection
  data_collection:
    episodes: 1000
    max_steps_per_episode: 1000
    save_frames: true
    frame_skip: 1
  
  # CNN architecture
  cnn:
    input_channels: 1  # 1 for grayscale, 3 for RGB
    conv_layers: [32, 64, 128, 256]
    fc_layers: [512, 256, 128]
    dropout_rate: 0.3
    activation: "relu"
  
  # Training parameters
  training:
    batch_size: 32
    learning_rate: 0.001
    epochs: 100
    validation_split: 0.2
    early_stopping_patience: 10
    optimizer: "adam"
    loss_function: "mse"
  
  # Data augmentation
  augmentation:
    enabled: true
    rotation_range: 5.0
    brightness_range: 0.1
    noise_std: 0.01
    blur_probability: 0.1

# Reinforcement Learning Settings
reinforcement_learning:
  # PPO parameters
  ppo:
    learning_rate: 0.0003
    n_steps: 2048
    batch_size: 64
    n_epochs: 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    clip_range_vf: null
    normalize_advantage: true
    ent_coef: 0.01
    vf_coef: 0.5
    max_grad_norm: 0.5
    use_sde: false
    sde_sample_freq: -1
    target_kl: null
  
  # Policy network
  policy_network:
    # For vision-based input
    cnn:
      input_channels: 1
      conv_layers: [32, 64, 128, 256]
      fc_layers: [512, 256]
      dropout_rate: 0.3
    
    # For state-based input
    mlp:
      hidden_layers: [256, 256, 128]
      activation: "relu"
      dropout_rate: 0.1
  
  # Training parameters
  training:
    total_timesteps: 1000000
    eval_freq: 10000
    n_eval_episodes: 10
    save_freq: 50000
    log_interval: 10

# Domain Randomization Settings
domain_randomization:
  enabled: true
  
  # Traffic randomization
  traffic:
    vehicle_count_range: [30, 70]
    speed_range: [15.0, 35.0]
    aggression_range: [0.1, 0.9]
    vehicle_types: ["IDMVehicle", "AggressiveVehicle", "DefensiveVehicle"]
  
  # Weather/visual randomization
  weather:
    brightness_range: [0.7, 1.3]
    contrast_range: [0.8, 1.2]
    noise_std_range: [0.0, 0.05]
    blur_range: [0.0, 2.0]
  
  # Road randomization
  road:
    lane_width_range: [3.5, 4.5]
    speed_limit_range: [25.0, 35.0]
    road_curvature_range: [0.0, 0.1]

# Multi-Agent Settings
multi_agent:
  enabled: true
  
  # Agent types
  agent_types:
    - name: "IDMVehicle"
      probability: 0.6
      speed_range: [20.0, 30.0]
    
    - name: "AggressiveVehicle"
      probability: 0.2
      speed_range: [25.0, 35.0]
    
    - name: "DefensiveVehicle"
      probability: 0.2
      speed_range: [15.0, 25.0]
  
  # Interaction settings
  interaction:
    collision_avoidance: true
    lane_change_aggression: 0.3
    merging_behavior: "cooperative"

# Reward Shaping
reward_shaping:
  # Basic rewards
  collision: -100
  off_road: -50
  right_lane: 1.0
  high_speed: 0.5
  speed_efficiency: 0.3
  
  # Safety rewards
  safe_distance: 0.2
  smooth_driving: 0.1
  lane_keeping: 0.5
  
  # Efficiency rewards
  progress: 0.1
  fuel_efficiency: 0.05
  
  # Penalties
  sudden_braking: -0.5
  sudden_acceleration: -0.3
  lane_violation: -1.0

# Evaluation Settings
evaluation:
  # Test scenarios
  scenarios:
    - name: "highway_normal"
      env: "highway-v0"
      vehicles_count: 50
      duration: 40
    
    - name: "highway_dense"
      env: "highway-v0"
      vehicles_count: 80
      duration: 40
    
    - name: "intersection"
      env: "intersection-v0"
      vehicles_count: 30
      duration: 40
    
    - name: "roundabout"
      env: "roundabout-v0"
      vehicles_count: 20
      duration: 40
    
    - name: "parking"
      env: "parking-v0"
      vehicles_count: 10
      duration: 60
  
  # Metrics
  metrics:
    - "episode_reward"
    - "collision_rate"
    - "average_speed"
    - "lane_keeping_score"
    - "safety_score"
    - "efficiency_score"
  
  # Evaluation parameters
  n_eval_episodes: 50
  render_eval: true
  save_videos: true

# Logging and Visualization
logging:
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: "./logs"
    update_freq: 100
  
  # Weights & Biases
  wandb:
    enabled: false
    project_name: "autonomous-driving-agent"
    entity: null
  
  # Model saving
  model_saving:
    save_dir: "./models"
    save_best_only: true
    save_frequency: 10000
  
  # Visualization
  visualization:
    enabled: true
    plot_frequency: 1000
    save_plots: true
    plot_dir: "./plots"